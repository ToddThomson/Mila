<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Mila: Dnn.Modules.Attention Module Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Mila
   </div>
   <div id="projectbrief">Deep Neural Network Library</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('module_Dnn_8Modules_8Attention.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#classes">Classes</a> &#124;
<a href="#typedef-members">Typedefs</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a> &#124;
<a href="#files">Files</a>  </div>
  <div class="headertitle"><div class="title">Dnn.Modules.Attention Module Reference</div></div>
</div><!--header-->
<div class="contents">
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="exports" name="exports"></a>
Exported Modules</h2></td></tr>
<tr class="memitem:Compute.OperationAttributes" id="r_Compute.OperationAttributes"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8OperationAttributes.html">Compute.OperationAttributes</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.CudaDevice" id="r_Compute.CudaDevice"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8CudaDevice.html">Compute.CudaDevice</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Serialization.ModelArchive" id="r_Serialization.ModelArchive"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Serialization_8ModelArchive.html">Serialization.ModelArchive</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.OperationRegistry" id="r_Compute.OperationRegistry"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8OperationRegistry.html">Compute.OperationRegistry</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.Precision" id="r_Compute.Precision"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8Precision.html">Compute.Precision</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Dnn.TensorTraits" id="r_Dnn.TensorTraits"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Dnn_8TensorTraits.html">Dnn.TensorTraits</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.MemoryResource" id="r_Compute.MemoryResource"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8MemoryResource.html">Compute.MemoryResource</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.DeviceType" id="r_Compute.DeviceType"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8DeviceType.html">Compute.DeviceType</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Dnn.Module" id="r_Dnn.Module"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Dnn_8Module.html">Dnn.Module</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Dnn.Tensor" id="r_Dnn.Tensor"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Dnn_8Tensor.html">Dnn.Tensor</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Dnn.TensorHelpers" id="r_Dnn.TensorHelpers"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Dnn_8TensorHelpers.html">Dnn.TensorHelpers</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.CpuMemoryResource" id="r_Compute.CpuMemoryResource"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8CpuMemoryResource.html">Compute.CpuMemoryResource</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.OperationBase" id="r_Compute.OperationBase"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8OperationBase.html">Compute.OperationBase</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.UnaryOperation" id="r_Compute.UnaryOperation"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8UnaryOperation.html">Compute.UnaryOperation</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.CpuDevice" id="r_Compute.CpuDevice"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8CpuDevice.html">Compute.CpuDevice</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.CudaMemoryResource" id="r_Compute.CudaMemoryResource"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8CudaMemoryResource.html">Compute.CudaMemoryResource</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:Compute.DeviceContext" id="r_Compute.DeviceContext"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="module_Compute_8DeviceContext.html">Compute.DeviceContext</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html">Mila::Dnn::MultiHeadAttention&lt; TDeviceType, TInput, TOutput &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multi-head attention module for transformer architectures.  <a href="classMila_1_1Dnn_1_1MultiHeadAttention.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttentionConfig.html">Mila::Dnn::MultiHeadAttentionConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html" title="Multi-head attention module for transformer architectures.">MultiHeadAttention</a> module.  <a href="classMila_1_1Dnn_1_1MultiHeadAttentionConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="typedef-members" name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:a9b2116c65236674a435be3299e8ace6e" id="r_a9b2116c65236674a435be3299e8ace6e"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:a9b2116c65236674a435be3299e8ace6e"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a9b2116c65236674a435be3299e8ace6e">Mila::Dnn::CpuMultiHeadAttention</a> = MultiHeadAttention&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a9b2116c65236674a435be3299e8ace6e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based multi-head attention module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a9b2116c65236674a435be3299e8ace6e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aadfb6cc6bf7e0df44ea0632a1c9243a0" id="r_aadfb6cc6bf7e0df44ea0632a1c9243a0"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:aadfb6cc6bf7e0df44ea0632a1c9243a0"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aadfb6cc6bf7e0df44ea0632a1c9243a0">Mila::Dnn::CudaMultiHeadAttention</a> = MultiHeadAttention&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:aadfb6cc6bf7e0df44ea0632a1c9243a0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based multi-head attention module with customizable tensor types.  <br /></td></tr>
<tr class="separator:aadfb6cc6bf7e0df44ea0632a1c9243a0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9fea4bf3612d224d2b54da7a1cb78cb8" id="r_a9fea4bf3612d224d2b54da7a1cb78cb8"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a9fea4bf3612d224d2b54da7a1cb78cb8">ModuleBase</a> = Module&lt; TDeviceType, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a9fea4bf3612d224d2b54da7a1cb78cb8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Alias for base module type.  <br /></td></tr>
<tr class="separator:a9fea4bf3612d224d2b54da7a1cb78cb8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a44c10df27cce6d2a46dbb81e455d5d4a" id="r_a44c10df27cce6d2a46dbb81e455d5d4a"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a44c10df27cce6d2a46dbb81e455d5d4a">MR</a> = std::conditional_t&lt; TDeviceType==DeviceType::Cuda, CudaMemoryResource, CpuMemoryResource &gt;</td></tr>
<tr class="memdesc:a44c10df27cce6d2a46dbb81e455d5d4a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Memory resource type used for tensors, selected based on device type.  <br /></td></tr>
<tr class="separator:a44c10df27cce6d2a46dbb81e455d5d4a"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a205a226fb09b4c4b083e60e1d4b4311a" id="r_a205a226fb09b4c4b083e60e1d4b4311a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a205a226fb09b4c4b083e60e1d4b4311a">MultiHeadAttention</a> (const std::string &amp;device_name, const MultiHeadAttentionConfig &amp;config)</td></tr>
<tr class="memdesc:a205a226fb09b4c4b083e60e1d4b4311a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructs a new <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html" title="Multi-head attention module for transformer architectures.">MultiHeadAttention</a> module with a device name.  <br /></td></tr>
<tr class="separator:a205a226fb09b4c4b083e60e1d4b4311a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2184c5fdc3dffb5dbbe2ba3f00ed48a9" id="r_a2184c5fdc3dffb5dbbe2ba3f00ed48a9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a2184c5fdc3dffb5dbbe2ba3f00ed48a9">MultiHeadAttention</a> (std::shared_ptr&lt; DeviceContext &gt; device_context, const MultiHeadAttentionConfig &amp;config)</td></tr>
<tr class="memdesc:a2184c5fdc3dffb5dbbe2ba3f00ed48a9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructs a new <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html" title="Multi-head attention module for transformer architectures.">MultiHeadAttention</a> module with a provided device context.  <br /></td></tr>
<tr class="separator:a2184c5fdc3dffb5dbbe2ba3f00ed48a9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abe9c2244b3ec8fd5ff188d26da34c8e9" id="r_abe9c2244b3ec8fd5ff188d26da34c8e9"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#abe9c2244b3ec8fd5ff188d26da34c8e9">backward</a> (const Tensor&lt; TInput, MR &gt; &amp;input, const Tensor&lt; TOutput, MR &gt; &amp;output_grad, Tensor&lt; TInput, MR &gt; &amp;input_grad)</td></tr>
<tr class="memdesc:abe9c2244b3ec8fd5ff188d26da34c8e9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs the backward pass of the <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html" title="Multi-head attention module for transformer architectures.">MultiHeadAttention</a> operation.  <br /></td></tr>
<tr class="separator:abe9c2244b3ec8fd5ff188d26da34c8e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa582f04abff968b547719d6a31320e5a" id="r_aa582f04abff968b547719d6a31320e5a"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#aa582f04abff968b547719d6a31320e5a">createOperation</a> ()</td></tr>
<tr class="memdesc:aa582f04abff968b547719d6a31320e5a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Creates the appropriate attention operation for the current device.  <br /></td></tr>
<tr class="separator:aa582f04abff968b547719d6a31320e5a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a825e50e3cc60ac78310129b056904810" id="r_a825e50e3cc60ac78310129b056904810"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a825e50e3cc60ac78310129b056904810">forward</a> (const Tensor&lt; TInput, MR &gt; &amp;input, const Tensor&lt; TInput, MR &gt; &amp;mask, Tensor&lt; TOutput, MR &gt; &amp;output)</td></tr>
<tr class="memdesc:a825e50e3cc60ac78310129b056904810"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs the forward pass with an explicit attention mask.  <br /></td></tr>
<tr class="separator:a825e50e3cc60ac78310129b056904810"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa68240bd5a909f19afe06e497f50a9aa" id="r_aa68240bd5a909f19afe06e497f50a9aa"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#aa68240bd5a909f19afe06e497f50a9aa">forward</a> (const Tensor&lt; TInput, MR &gt; &amp;input, Tensor&lt; TOutput, MR &gt; &amp;output)</td></tr>
<tr class="memdesc:aa68240bd5a909f19afe06e497f50a9aa"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs the forward pass of the <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html" title="Multi-head attention module for transformer architectures.">MultiHeadAttention</a> operation.  <br /></td></tr>
<tr class="separator:aa68240bd5a909f19afe06e497f50a9aa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a73b8961371a4f0c0881b08aa0a5de198" id="r_a73b8961371a4f0c0881b08aa0a5de198"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a73b8961371a4f0c0881b08aa0a5de198">getDropout</a> () const</td></tr>
<tr class="memdesc:a73b8961371a4f0c0881b08aa0a5de198"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the dropout rate.  <br /></td></tr>
<tr class="separator:a73b8961371a4f0c0881b08aa0a5de198"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa3b867c5cfaeeef2936fb4ca46ff61ff" id="r_aa3b867c5cfaeeef2936fb4ca46ff61ff"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#aa3b867c5cfaeeef2936fb4ca46ff61ff">getEmbeddingDim</a> () const</td></tr>
<tr class="memdesc:aa3b867c5cfaeeef2936fb4ca46ff61ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the embedding dimension.  <br /></td></tr>
<tr class="separator:aa3b867c5cfaeeef2936fb4ca46ff61ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a21b2cafa427d56474aea9bf5deffa2ed" id="r_a21b2cafa427d56474aea9bf5deffa2ed"><td class="memItemLeft" align="right" valign="top">const std::vector&lt; size_t &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a21b2cafa427d56474aea9bf5deffa2ed">getInputShape</a> () const</td></tr>
<tr class="memdesc:a21b2cafa427d56474aea9bf5deffa2ed"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the input shape.  <br /></td></tr>
<tr class="separator:a21b2cafa427d56474aea9bf5deffa2ed"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8a27412f9f4f9e60fa2c6a569c59167f" id="r_a8a27412f9f4f9e60fa2c6a569c59167f"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a8a27412f9f4f9e60fa2c6a569c59167f">getNumHeads</a> () const</td></tr>
<tr class="memdesc:a8a27412f9f4f9e60fa2c6a569c59167f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the number of attention heads.  <br /></td></tr>
<tr class="separator:a8a27412f9f4f9e60fa2c6a569c59167f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2e09b883e547c95eee3b1f26b212c2f" id="r_aa2e09b883e547c95eee3b1f26b212c2f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#aa2e09b883e547c95eee3b1f26b212c2f">initializeTensors</a> ()</td></tr>
<tr class="memdesc:aa2e09b883e547c95eee3b1f26b212c2f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initializes the tensors needed for attention computation.  <br /></td></tr>
<tr class="separator:aa2e09b883e547c95eee3b1f26b212c2f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4c16e1be1d56f6eebb1b969e5bbc6ca" id="r_ab4c16e1be1d56f6eebb1b969e5bbc6ca"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#ab4c16e1be1d56f6eebb1b969e5bbc6ca">load</a> (ModelArchive &amp;archive) override</td></tr>
<tr class="memdesc:ab4c16e1be1d56f6eebb1b969e5bbc6ca"><td class="mdescLeft">&#160;</td><td class="mdescRight">Loads the module state from a ZIP archive.  <br /></td></tr>
<tr class="separator:ab4c16e1be1d56f6eebb1b969e5bbc6ca"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d7f1415090f6ce3f84a492d72a93504" id="r_a7d7f1415090f6ce3f84a492d72a93504"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a7d7f1415090f6ce3f84a492d72a93504">parameterCount</a> () const override</td></tr>
<tr class="memdesc:a7d7f1415090f6ce3f84a492d72a93504"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the number of trainable parameters in this module.  <br /></td></tr>
<tr class="separator:a7d7f1415090f6ce3f84a492d72a93504"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aae30a0f8075da1f957316a0a0c0643d7" id="r_aae30a0f8075da1f957316a0a0c0643d7"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#aae30a0f8075da1f957316a0a0c0643d7">save</a> (ModelArchive &amp;zip) const override</td></tr>
<tr class="memdesc:aae30a0f8075da1f957316a0a0c0643d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Saves the module state to a ZIP archive.  <br /></td></tr>
<tr class="separator:aae30a0f8075da1f957316a0a0c0643d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a91762121adfdf91a746f16619b26fb98" id="r_a91762121adfdf91a746f16619b26fb98"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a91762121adfdf91a746f16619b26fb98">toString</a> () const override</td></tr>
<tr class="memdesc:a91762121adfdf91a746f16619b26fb98"><td class="mdescLeft">&#160;</td><td class="mdescRight">Generates a string representation of this module's configuration.  <br /></td></tr>
<tr class="separator:a91762121adfdf91a746f16619b26fb98"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac0b829536527ba668c4180f67aa0f874" id="r_ac0b829536527ba668c4180f67aa0f874"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#ac0b829536527ba668c4180f67aa0f874">useCausalMask</a> () const</td></tr>
<tr class="memdesc:ac0b829536527ba668c4180f67aa0f874"><td class="mdescLeft">&#160;</td><td class="mdescRight">Checks if causal masking is enabled.  <br /></td></tr>
<tr class="separator:ac0b829536527ba668c4180f67aa0f874"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:ad6a96211088c452ecad01c26c4abf1ab" id="r_ad6a96211088c452ecad01c26c4abf1ab"><td class="memItemLeft" align="right" valign="top">std::shared_ptr&lt; Tensor&lt; TOutput, MR &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#ad6a96211088c452ecad01c26c4abf1ab">attn_</a> { nullptr }</td></tr>
<tr class="memdesc:ad6a96211088c452ecad01c26c4abf1ab"><td class="mdescLeft">&#160;</td><td class="mdescRight">Attention weight tensor from the forward pass.  <br /></td></tr>
<tr class="separator:ad6a96211088c452ecad01c26c4abf1ab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0fe952f1cba9b123c7ff402fb1f0e72" id="r_ad0fe952f1cba9b123c7ff402fb1f0e72"><td class="memItemLeft" align="right" valign="top">MultiHeadAttentionConfig&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#ad0fe952f1cba9b123c7ff402fb1f0e72">config_</a></td></tr>
<tr class="memdesc:ad0fe952f1cba9b123c7ff402fb1f0e72"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration for the <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html" title="Multi-head attention module for transformer architectures.">MultiHeadAttention</a> module.  <br /></td></tr>
<tr class="separator:ad0fe952f1cba9b123c7ff402fb1f0e72"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3c4375367d92713af8837672b3c7780" id="r_ab3c4375367d92713af8837672b3c7780"><td class="memItemLeft" align="right" valign="top">std::shared_ptr&lt; UnaryOperation&lt; TDeviceType, TInput, TOutput &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#ab3c4375367d92713af8837672b3c7780">operation_</a> { nullptr }</td></tr>
<tr class="memdesc:ab3c4375367d92713af8837672b3c7780"><td class="mdescLeft">&#160;</td><td class="mdescRight">The operation that implements the attention mechanism.  <br /></td></tr>
<tr class="separator:ab3c4375367d92713af8837672b3c7780"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab56f25b08a4d71130c54e35b17a67690" id="r_ab56f25b08a4d71130c54e35b17a67690"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::shared_ptr&lt; Tensor&lt; TOutput, MR &gt; &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#ab56f25b08a4d71130c54e35b17a67690">output_state_</a></td></tr>
<tr class="memdesc:ab56f25b08a4d71130c54e35b17a67690"><td class="mdescLeft">&#160;</td><td class="mdescRight">Collection of output state tensors for caching.  <br /></td></tr>
<tr class="separator:ab56f25b08a4d71130c54e35b17a67690"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a54da4678bf30ea520cbf8070268cce18" id="r_a54da4678bf30ea520cbf8070268cce18"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::shared_ptr&lt; Tensor&lt; TOutput, MR &gt; &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a54da4678bf30ea520cbf8070268cce18">parameters_</a></td></tr>
<tr class="memdesc:a54da4678bf30ea520cbf8070268cce18"><td class="mdescLeft">&#160;</td><td class="mdescRight">Collection of parameters for this module.  <br /></td></tr>
<tr class="separator:a54da4678bf30ea520cbf8070268cce18"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae5a3ffa78b72bf120232f11ece348c7a" id="r_ae5a3ffa78b72bf120232f11ece348c7a"><td class="memItemLeft" align="right" valign="top">std::shared_ptr&lt; Tensor&lt; TOutput, MR &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#ae5a3ffa78b72bf120232f11ece348c7a">pre_attn_</a> { nullptr }</td></tr>
<tr class="memdesc:ae5a3ffa78b72bf120232f11ece348c7a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Pre-softmax attention scores from the forward pass.  <br /></td></tr>
<tr class="separator:ae5a3ffa78b72bf120232f11ece348c7a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a62af7179a0cced02ded53601d4b50ce1" id="r_a62af7179a0cced02ded53601d4b50ce1"><td class="memItemLeft" align="right" valign="top">OperationAttributes&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html#a62af7179a0cced02ded53601d4b50ce1">properties_</a></td></tr>
<tr class="memdesc:a62af7179a0cced02ded53601d4b50ce1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Operation attributes and configuration.  <br /></td></tr>
<tr class="separator:a62af7179a0cced02ded53601d4b50ce1"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="files" name="files"></a>
Files</h2></td></tr>
<tr class="memitem:MultiHeadAttention_8ixx" id="r_MultiHeadAttention_8ixx"><td class="memItemLeft" align="right" valign="top">file &#160;</td><td class="memItemRight" valign="bottom">/home/runner/work/Mila/Mila/Mila/Src/Dnn/Modules/Layers/<a class="el" href="MultiHeadAttention_8ixx.html">MultiHeadAttention.ixx</a></td></tr>
<tr class="memdesc:MultiHeadAttention_8ixx"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of multi-head attention mechanism for transformer architectures. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:MultiHeadAttentionConfig_8ixx" id="r_MultiHeadAttentionConfig_8ixx"><td class="memItemLeft" align="right" valign="top">file &#160;</td><td class="memItemRight" valign="bottom">/home/runner/work/Mila/Mila/Mila/Src/Dnn/Modules/Layers/<a class="el" href="MultiHeadAttentionConfig_8ixx.html">MultiHeadAttentionConfig.ixx</a></td></tr>
<tr class="memdesc:MultiHeadAttentionConfig_8ixx"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration interface for the MultiHeadAttention module in the <a class="el" href="namespaceMila.html">Mila</a> DNN framework. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8 </li>
  </ul>
</div>
</body>
</html>
