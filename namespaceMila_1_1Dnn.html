<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Mila: Mila::Dnn Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Mila
   </div>
   <div id="projectbrief">Deep Neural Network Library</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('namespaceMila_1_1Dnn.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a> &#124;
<a href="#nested-classes">Classes</a> &#124;
<a href="#concepts">Concepts</a> &#124;
<a href="#typedef-members">Typedefs</a> &#124;
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">Mila::Dnn Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="namespaces" name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespaceMila_1_1Dnn_1_1Compute" id="r_namespaceMila_1_1Dnn_1_1Compute"><td class="memItemLeft" align="right" valign="top">namespace &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn_1_1Compute.html">Compute</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespaceMila_1_1Dnn_1_1Data" id="r_namespaceMila_1_1Dnn_1_1Data"><td class="memItemLeft" align="right" valign="top">namespace &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespaceMila_1_1Dnn_1_1detail" id="r_namespaceMila_1_1Dnn_1_1detail"><td class="memItemLeft" align="right" valign="top">namespace &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn_1_1detail.html">detail</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespaceMila_1_1Dnn_1_1Gpt2" id="r_namespaceMila_1_1Dnn_1_1Gpt2"><td class="memItemLeft" align="right" valign="top">namespace &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn_1_1Gpt2.html">Gpt2</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespaceMila_1_1Dnn_1_1Serialization" id="r_namespaceMila_1_1Dnn_1_1Serialization"><td class="memItemLeft" align="right" valign="top">namespace &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn_1_1Serialization.html">Serialization</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespaceMila_1_1Dnn_1_1Utils" id="r_namespaceMila_1_1Dnn_1_1Utils"><td class="memItemLeft" align="right" valign="top">namespace &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn_1_1Utils.html">Utils</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Component.html">Component</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Abstract base class for all components in the <a class="el" href="namespaceMila.html">Mila</a> framework.  <a href="classMila_1_1Dnn_1_1Component.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1ComponentConfig.html">ComponentConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Base configuration class for all neural network components.  <a href="classMila_1_1Dnn_1_1ComponentConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1CompositeModule.html">CompositeModule</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A module class that can contain and manage child modules.  <a href="classMila_1_1Dnn_1_1CompositeModule.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1CrossEntropy.html">CrossEntropy</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classMila_1_1Dnn_1_1CrossEntropy.html" title="CrossEntropy loss module for neural networks.">CrossEntropy</a> loss module for neural networks.  <a href="classMila_1_1Dnn_1_1CrossEntropy.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1CrossEntropyConfig.html">CrossEntropyConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1CrossEntropy.html" title="CrossEntropy loss module for neural networks.">CrossEntropy</a> module.  <a href="classMila_1_1Dnn_1_1CrossEntropyConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Dropout.html">Dropout</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classMila_1_1Dnn_1_1Dropout.html" title="Dropout regularization module for neural networks.">Dropout</a> regularization module for neural networks.  <a href="classMila_1_1Dnn_1_1Dropout.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1DropoutConfig.html">DropoutConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1Dropout.html" title="Dropout regularization module for neural networks.">Dropout</a> module.  <a href="classMila_1_1Dnn_1_1DropoutConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Encoder.html">Encoder</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">An encoder module that provides token and positional embeddings.  <a href="classMila_1_1Dnn_1_1Encoder.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1EncoderConfig.html">EncoderConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1Encoder.html" title="An encoder module that provides token and positional embeddings.">Encoder</a> module.  <a href="classMila_1_1Dnn_1_1EncoderConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1FusedModule.html">FusedModule</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Gelu.html">Gelu</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gaussian Error <a class="el" href="classMila_1_1Dnn_1_1Linear.html" title="A class representing a linear transformation module.">Linear</a> Unit (GELU) activation function module.  <a href="classMila_1_1Dnn_1_1Gelu.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1GeluConfig.html">GeluConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for GELU module.  <a href="classMila_1_1Dnn_1_1GeluConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1LayerNorm.html">LayerNorm</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Layer Normalization module.  <a href="classMila_1_1Dnn_1_1LayerNorm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1LayerNormConfig.html">LayerNormConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for Layer Normalization module.  <a href="classMila_1_1Dnn_1_1LayerNormConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Linear.html">Linear</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A class representing a linear transformation module.  <a href="classMila_1_1Dnn_1_1Linear.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1LinearConfig.html">LinearConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1Linear.html" title="A class representing a linear transformation module.">Linear</a> module.  <a href="classMila_1_1Dnn_1_1LinearConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MLP.html">MLP</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multi-Layer Perceptron (<a class="el" href="classMila_1_1Dnn_1_1MLP.html" title="Multi-Layer Perceptron (MLP) block for neural networks.">MLP</a>) block for neural networks.  <a href="classMila_1_1Dnn_1_1MLP.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MLPConfig.html">MLPConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1MLP.html" title="Multi-Layer Perceptron (MLP) block for neural networks.">MLP</a> block.  <a href="classMila_1_1Dnn_1_1MLPConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Model.html">Model</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A class representing a neural network model.  <a href="classMila_1_1Dnn_1_1Model.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1ModelCallback.html">ModelCallback</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface for callbacks during training.  <a href="classMila_1_1Dnn_1_1ModelCallback.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Module.html">Module</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Abstract base class for all modules in the <a class="el" href="namespaceMila.html">Mila</a> DNN framework.  <a href="classMila_1_1Dnn_1_1Module.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html">MultiHeadAttention</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multi-head attention module for transformer architectures.  <a href="classMila_1_1Dnn_1_1MultiHeadAttention.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttentionConfig.html">MultiHeadAttentionConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html" title="Multi-head attention module for transformer architectures.">MultiHeadAttention</a> module.  <a href="classMila_1_1Dnn_1_1MultiHeadAttentionConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Residual.html">Residual</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A class implementing a residual connection module.  <a href="classMila_1_1Dnn_1_1Residual.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1ResidualConfig.html">ResidualConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1Residual.html" title="A class implementing a residual connection module.">Residual</a> connection module.  <a href="classMila_1_1Dnn_1_1ResidualConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Softmax.html">Softmax</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classMila_1_1Dnn_1_1Softmax.html" title="Softmax module for neural networks.">Softmax</a> module for neural networks.  <a href="classMila_1_1Dnn_1_1Softmax.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1SoftmaxConfig.html">SoftmaxConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1Softmax.html" title="Softmax module for neural networks.">Softmax</a> module.  <a href="classMila_1_1Dnn_1_1SoftmaxConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1TensorBuffer.html">TensorBuffer</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A buffer for storing tensor data with configurable memory management.  <a href="classMila_1_1Dnn_1_1TensorBuffer.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html">TensorPtr</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Base tensor pointer class that wraps a raw pointer with memory-type safety.  <a href="classMila_1_1Dnn_1_1TensorPtr.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html">TensorTrait</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Primary template for tensor type traits.  <a href="structMila_1_1Dnn_1_1TensorTrait.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01____nv__fp8__e4m3_01_4.html">TensorTrait&lt; __nv_fp8_e4m3 &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for 8-bit floating point type (e4m3).  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01____nv__fp8__e4m3_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01____nv__fp8__e5m2_01_4.html">TensorTrait&lt; __nv_fp8_e5m2 &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for alternative 8-bit floating point type (e5m2).  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01____nv__fp8__e5m2_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01float_01_4.html">TensorTrait&lt; float &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for float type.  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01float_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01half_01_4.html">TensorTrait&lt; half &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for half-precision float type.  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01half_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01int_01_4.html">TensorTrait&lt; int &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for 32-bit signed integer type.  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01int_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01int16__t_01_4.html">TensorTrait&lt; int16_t &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for 16-bit signed integer type.  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01int16__t_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01nv__bfloat16_01_4.html">TensorTrait&lt; nv_bfloat16 &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for NVIDIA bfloat16 type.  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01nv__bfloat16_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01uint16__t_01_4.html">TensorTrait&lt; uint16_t &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for 16-bit unsigned integer type.  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01uint16__t_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TensorTrait_3_01uint32__t_01_4.html">TensorTrait&lt; uint32_t &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specialization of <a class="el" href="structMila_1_1Dnn_1_1TensorTrait.html" title="Primary template for tensor type traits.">TensorTrait</a> for 32-bit unsigned integer type.  <a href="structMila_1_1Dnn_1_1TensorTrait_3_01uint32__t_01_4.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structMila_1_1Dnn_1_1TrainingConfig.html">TrainingConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration for training a model.  <a href="structMila_1_1Dnn_1_1TrainingConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1TransformerBlock.html">TransformerBlock</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classMila_1_1Dnn_1_1TransformerBlock.html" title="TransformerBlock implements a standard transformer encoder block.">TransformerBlock</a> implements a standard transformer encoder block.  <a href="classMila_1_1Dnn_1_1TransformerBlock.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1TransformerBlockConfig.html">TransformerBlockConfig</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Configuration class for <a class="el" href="classMila_1_1Dnn_1_1TransformerBlock.html" title="TransformerBlock implements a standard transformer encoder block.">TransformerBlock</a>.  <a href="classMila_1_1Dnn_1_1TransformerBlockConfig.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMila_1_1Dnn_1_1UniqueIdGenerator.html">UniqueIdGenerator</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="concepts" name="concepts"></a>
Concepts</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">concept &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="conceptMila_1_1Dnn_1_1ValidTensorType.html">ValidTensorType</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Concept that constrains types to those with valid tensor trait specializations. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">concept &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="conceptMila_1_1Dnn_1_1ValidFloatTensorType.html">ValidFloatTensorType</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Concept that constrains types to valid floating-point tensor types. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">concept &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="conceptMila_1_1Dnn_1_1ValidFloatTensorTypes.html">ValidFloatTensorTypes</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Concept that verifies both types are valid floating-point tensor types. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">concept &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="conceptMila_1_1Dnn_1_1ValidTensorTypes.html">ValidTensorTypes</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Concept that verifies both input and compute types have valid tensor trait mappings. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="typedef-members" name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:aa220de50dd4be66d488fc2fb76b6b5de" id="r_aa220de50dd4be66d488fc2fb76b6b5de"><td class="memTemplParams" colspan="2">template&lt;typename TDataType  = float&gt; </td></tr>
<tr class="memitem:aa220de50dd4be66d488fc2fb76b6b5de"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aa220de50dd4be66d488fc2fb76b6b5de">Mila::Dnn::CpuCompositeModule</a> = <a class="el" href="classMila_1_1Dnn_1_1CompositeModule.html">CompositeModule</a>&lt; DeviceType::Cpu, TDataType &gt;</td></tr>
<tr class="separator:aa220de50dd4be66d488fc2fb76b6b5de"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4dd9edcd0c4bd13d63d7fbdc8e5886af" id="r_a4dd9edcd0c4bd13d63d7fbdc8e5886af"><td class="memTemplParams" colspan="2">template&lt;typename TLogits  = float, typename TTargets  = int&gt; </td></tr>
<tr class="memitem:a4dd9edcd0c4bd13d63d7fbdc8e5886af"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a4dd9edcd0c4bd13d63d7fbdc8e5886af">Mila::Dnn::CpuCrossEntropy</a> = <a class="el" href="classMila_1_1Dnn_1_1CrossEntropy.html">CrossEntropy</a>&lt; DeviceType::Cpu, TLogits, TTargets &gt;</td></tr>
<tr class="memdesc:a4dd9edcd0c4bd13d63d7fbdc8e5886af"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based cross entropy module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a4dd9edcd0c4bd13d63d7fbdc8e5886af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1f196b598ccd007e5aadc2328089f847" id="r_a1f196b598ccd007e5aadc2328089f847"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:a1f196b598ccd007e5aadc2328089f847"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a1f196b598ccd007e5aadc2328089f847">Mila::Dnn::CpuDropout</a> = <a class="el" href="classMila_1_1Dnn_1_1Dropout.html">Dropout</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a1f196b598ccd007e5aadc2328089f847"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based dropout module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a1f196b598ccd007e5aadc2328089f847"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a40ef87d5b79c7b0dee7cbc489507c8b3" id="r_a40ef87d5b79c7b0dee7cbc489507c8b3"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = int, typename TOutput  = float&gt; </td></tr>
<tr class="memitem:a40ef87d5b79c7b0dee7cbc489507c8b3"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a40ef87d5b79c7b0dee7cbc489507c8b3">Mila::Dnn::CpuEncoder</a> = <a class="el" href="classMila_1_1Dnn_1_1Encoder.html">Encoder</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a40ef87d5b79c7b0dee7cbc489507c8b3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based encoder module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a40ef87d5b79c7b0dee7cbc489507c8b3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa5a5e7e259089aa0242b266ff0d67a8a" id="r_aa5a5e7e259089aa0242b266ff0d67a8a"><td class="memTemplParams" colspan="2">template&lt;typename TDataType  = float&gt; </td></tr>
<tr class="memitem:aa5a5e7e259089aa0242b266ff0d67a8a"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aa5a5e7e259089aa0242b266ff0d67a8a">Mila::Dnn::CpuGelu</a> = <a class="el" href="classMila_1_1Dnn_1_1Gelu.html">Gelu</a>&lt; DeviceType::Cpu, TDataType &gt;</td></tr>
<tr class="memdesc:aa5a5e7e259089aa0242b266ff0d67a8a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-specific GELU module.  <br /></td></tr>
<tr class="separator:aa5a5e7e259089aa0242b266ff0d67a8a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aae4780e5db165f896a415e33a6cb5498" id="r_aae4780e5db165f896a415e33a6cb5498"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:aae4780e5db165f896a415e33a6cb5498"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aae4780e5db165f896a415e33a6cb5498">Mila::Dnn::CpuLayerNorm</a> = <a class="el" href="classMila_1_1Dnn_1_1LayerNorm.html">LayerNorm</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:aae4780e5db165f896a415e33a6cb5498"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based layer normalization module with customizable tensor types.  <br /></td></tr>
<tr class="separator:aae4780e5db165f896a415e33a6cb5498"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae99a94869c61be63b8d7760e0bd82a18" id="r_ae99a94869c61be63b8d7760e0bd82a18"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:ae99a94869c61be63b8d7760e0bd82a18"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#ae99a94869c61be63b8d7760e0bd82a18">Mila::Dnn::CpuLinear</a> = <a class="el" href="classMila_1_1Dnn_1_1Linear.html">Linear</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:ae99a94869c61be63b8d7760e0bd82a18"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based linear module with customizable tensor types.  <br /></td></tr>
<tr class="separator:ae99a94869c61be63b8d7760e0bd82a18"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a9b95dc4b57e7427b3874f5e9464e04" id="r_a5a9b95dc4b57e7427b3874f5e9464e04"><td class="memTemplParams" colspan="2">template&lt;typename TDataType  = float&gt; </td></tr>
<tr class="memitem:a5a9b95dc4b57e7427b3874f5e9464e04"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a5a9b95dc4b57e7427b3874f5e9464e04">Mila::Dnn::CpuMLP</a> = <a class="el" href="classMila_1_1Dnn_1_1MLP.html">MLP</a>&lt; DeviceType::Cpu, TDataType &gt;</td></tr>
<tr class="memdesc:a5a9b95dc4b57e7427b3874f5e9464e04"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based <a class="el" href="classMila_1_1Dnn_1_1MLP.html" title="Multi-Layer Perceptron (MLP) block for neural networks.">MLP</a> module with customizable tensor type.  <br /></td></tr>
<tr class="separator:a5a9b95dc4b57e7427b3874f5e9464e04"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa8b6fafaecb0ac362383aa6ae4e3726c" id="r_aa8b6fafaecb0ac362383aa6ae4e3726c"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:aa8b6fafaecb0ac362383aa6ae4e3726c"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aa8b6fafaecb0ac362383aa6ae4e3726c">Mila::Dnn::CpuModel</a> = <a class="el" href="classMila_1_1Dnn_1_1Model.html">Model</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:aa8b6fafaecb0ac362383aa6ae4e3726c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based models with customizable tensor types.  <br /></td></tr>
<tr class="separator:aa8b6fafaecb0ac362383aa6ae4e3726c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adf61255fac3ae920623d526c30697905" id="r_adf61255fac3ae920623d526c30697905"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:adf61255fac3ae920623d526c30697905"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#adf61255fac3ae920623d526c30697905">Mila::Dnn::CpuModule</a> = <a class="el" href="classMila_1_1Dnn_1_1Module.html">Module</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:adf61255fac3ae920623d526c30697905"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based modules with customizable tensor types.  <br /></td></tr>
<tr class="separator:adf61255fac3ae920623d526c30697905"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9b2116c65236674a435be3299e8ace6e" id="r_a9b2116c65236674a435be3299e8ace6e"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:a9b2116c65236674a435be3299e8ace6e"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a9b2116c65236674a435be3299e8ace6e">Mila::Dnn::CpuMultiHeadAttention</a> = <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html">MultiHeadAttention</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a9b2116c65236674a435be3299e8ace6e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based multi-head attention module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a9b2116c65236674a435be3299e8ace6e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a10a8ee611a881909837671539c932388" id="r_a10a8ee611a881909837671539c932388"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:a10a8ee611a881909837671539c932388"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a10a8ee611a881909837671539c932388">Mila::Dnn::CpuResidual</a> = <a class="el" href="classMila_1_1Dnn_1_1Residual.html">Residual</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a10a8ee611a881909837671539c932388"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based residual module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a10a8ee611a881909837671539c932388"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8718f8ff56431457fba8f0003606e773" id="r_a8718f8ff56431457fba8f0003606e773"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:a8718f8ff56431457fba8f0003606e773"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a8718f8ff56431457fba8f0003606e773">Mila::Dnn::CpuSoftmax</a> = <a class="el" href="classMila_1_1Dnn_1_1Softmax.html">Softmax</a>&lt; DeviceType::Cpu, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a8718f8ff56431457fba8f0003606e773"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based softmax module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a8718f8ff56431457fba8f0003606e773"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3adb5958a1b0110973c2a4a08a3a4e33" id="r_a3adb5958a1b0110973c2a4a08a3a4e33"><td class="memTemplParams" colspan="2">template&lt;typename TDataType  = float&gt; </td></tr>
<tr class="memitem:a3adb5958a1b0110973c2a4a08a3a4e33"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a3adb5958a1b0110973c2a4a08a3a4e33">Mila::Dnn::CpuTransformerBlock</a> = <a class="el" href="classMila_1_1Dnn_1_1TransformerBlock.html">TransformerBlock</a>&lt; DeviceType::Cpu, TDataType &gt;</td></tr>
<tr class="memdesc:a3adb5958a1b0110973c2a4a08a3a4e33"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CPU-based transformer block with customizable tensor type.  <br /></td></tr>
<tr class="separator:a3adb5958a1b0110973c2a4a08a3a4e33"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c2bcc0d6541dc10d39803ab17be444b" id="r_a2c2bcc0d6541dc10d39803ab17be444b"><td class="memTemplParams" colspan="2">template&lt;typename TDataType  = float&gt; </td></tr>
<tr class="memitem:a2c2bcc0d6541dc10d39803ab17be444b"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a2c2bcc0d6541dc10d39803ab17be444b">Mila::Dnn::CudaCompositeModule</a> = <a class="el" href="classMila_1_1Dnn_1_1CompositeModule.html">CompositeModule</a>&lt; DeviceType::Cuda, TDataType &gt;</td></tr>
<tr class="separator:a2c2bcc0d6541dc10d39803ab17be444b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adc98eac63b0fbae2dee52f520cbd4cab" id="r_adc98eac63b0fbae2dee52f520cbd4cab"><td class="memTemplParams" colspan="2">template&lt;typename TLogits  = float, typename TTargets  = int&gt; </td></tr>
<tr class="memitem:adc98eac63b0fbae2dee52f520cbd4cab"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#adc98eac63b0fbae2dee52f520cbd4cab">Mila::Dnn::CudaCrossEntropy</a> = <a class="el" href="classMila_1_1Dnn_1_1CrossEntropy.html">CrossEntropy</a>&lt; DeviceType::Cuda, TLogits, TTargets &gt;</td></tr>
<tr class="memdesc:adc98eac63b0fbae2dee52f520cbd4cab"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based cross entropy module with customizable tensor types.  <br /></td></tr>
<tr class="separator:adc98eac63b0fbae2dee52f520cbd4cab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac54ac1ac346f0ff778124fb877b6b705" id="r_ac54ac1ac346f0ff778124fb877b6b705"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:ac54ac1ac346f0ff778124fb877b6b705"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#ac54ac1ac346f0ff778124fb877b6b705">Mila::Dnn::CudaDropout</a> = <a class="el" href="classMila_1_1Dnn_1_1Dropout.html">Dropout</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:ac54ac1ac346f0ff778124fb877b6b705"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based dropout module with customizable tensor types.  <br /></td></tr>
<tr class="separator:ac54ac1ac346f0ff778124fb877b6b705"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af04571a3ff5b48f119d4a3e7041a624d" id="r_af04571a3ff5b48f119d4a3e7041a624d"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = int, typename TOutput  = float&gt; </td></tr>
<tr class="memitem:af04571a3ff5b48f119d4a3e7041a624d"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#af04571a3ff5b48f119d4a3e7041a624d">Mila::Dnn::CudaEncoder</a> = <a class="el" href="classMila_1_1Dnn_1_1Encoder.html">Encoder</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:af04571a3ff5b48f119d4a3e7041a624d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based encoder module with customizable tensor types.  <br /></td></tr>
<tr class="separator:af04571a3ff5b48f119d4a3e7041a624d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8b1fe13574bd1bccef7e35c78a3cb552" id="r_a8b1fe13574bd1bccef7e35c78a3cb552"><td class="memTemplParams" colspan="2">template&lt;typename TDataType  = float&gt; </td></tr>
<tr class="memitem:a8b1fe13574bd1bccef7e35c78a3cb552"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a8b1fe13574bd1bccef7e35c78a3cb552">Mila::Dnn::CudaGelu</a> = <a class="el" href="classMila_1_1Dnn_1_1Gelu.html">Gelu</a>&lt; DeviceType::Cuda, TDataType &gt;</td></tr>
<tr class="memdesc:a8b1fe13574bd1bccef7e35c78a3cb552"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-specific GELU module.  <br /></td></tr>
<tr class="separator:a8b1fe13574bd1bccef7e35c78a3cb552"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aac3f80732055041758726aabd8de167b" id="r_aac3f80732055041758726aabd8de167b"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:aac3f80732055041758726aabd8de167b"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aac3f80732055041758726aabd8de167b">Mila::Dnn::CudaLayerNorm</a> = <a class="el" href="classMila_1_1Dnn_1_1LayerNorm.html">LayerNorm</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:aac3f80732055041758726aabd8de167b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based layer normalization module with customizable tensor types.  <br /></td></tr>
<tr class="separator:aac3f80732055041758726aabd8de167b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a542a877503e188fefa7c7f45f2511d4a" id="r_a542a877503e188fefa7c7f45f2511d4a"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:a542a877503e188fefa7c7f45f2511d4a"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a542a877503e188fefa7c7f45f2511d4a">Mila::Dnn::CudaLinear</a> = <a class="el" href="classMila_1_1Dnn_1_1Linear.html">Linear</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a542a877503e188fefa7c7f45f2511d4a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based linear module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a542a877503e188fefa7c7f45f2511d4a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5365ad3f092eab6c6a39bd7a6678b8ad" id="r_a5365ad3f092eab6c6a39bd7a6678b8ad"><td class="memTemplParams" colspan="2">template&lt;typename TDataType  = float&gt; </td></tr>
<tr class="memitem:a5365ad3f092eab6c6a39bd7a6678b8ad"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a5365ad3f092eab6c6a39bd7a6678b8ad">Mila::Dnn::CudaMLP</a> = <a class="el" href="classMila_1_1Dnn_1_1MLP.html">MLP</a>&lt; DeviceType::Cuda, TDataType &gt;</td></tr>
<tr class="memdesc:a5365ad3f092eab6c6a39bd7a6678b8ad"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based <a class="el" href="classMila_1_1Dnn_1_1MLP.html" title="Multi-Layer Perceptron (MLP) block for neural networks.">MLP</a> module with customizable tensor type.  <br /></td></tr>
<tr class="separator:a5365ad3f092eab6c6a39bd7a6678b8ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae1a82f21a2c7cb10ad057cae47f25af9" id="r_ae1a82f21a2c7cb10ad057cae47f25af9"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:ae1a82f21a2c7cb10ad057cae47f25af9"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#ae1a82f21a2c7cb10ad057cae47f25af9">Mila::Dnn::CudaModel</a> = <a class="el" href="classMila_1_1Dnn_1_1Model.html">Model</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:ae1a82f21a2c7cb10ad057cae47f25af9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based models with customizable tensor types.  <br /></td></tr>
<tr class="separator:ae1a82f21a2c7cb10ad057cae47f25af9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aae5155afe2c959b7e2d847e25bb0b4fd" id="r_aae5155afe2c959b7e2d847e25bb0b4fd"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:aae5155afe2c959b7e2d847e25bb0b4fd"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aae5155afe2c959b7e2d847e25bb0b4fd">Mila::Dnn::CudaModule</a> = <a class="el" href="classMila_1_1Dnn_1_1Module.html">Module</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:aae5155afe2c959b7e2d847e25bb0b4fd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based modules with customizable tensor types.  <br /></td></tr>
<tr class="separator:aae5155afe2c959b7e2d847e25bb0b4fd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aadfb6cc6bf7e0df44ea0632a1c9243a0" id="r_aadfb6cc6bf7e0df44ea0632a1c9243a0"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:aadfb6cc6bf7e0df44ea0632a1c9243a0"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aadfb6cc6bf7e0df44ea0632a1c9243a0">Mila::Dnn::CudaMultiHeadAttention</a> = <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html">MultiHeadAttention</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:aadfb6cc6bf7e0df44ea0632a1c9243a0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based multi-head attention module with customizable tensor types.  <br /></td></tr>
<tr class="separator:aadfb6cc6bf7e0df44ea0632a1c9243a0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad1ddaeb7c67d98fd987fdf83a93d087" id="r_aad1ddaeb7c67d98fd987fdf83a93d087"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:aad1ddaeb7c67d98fd987fdf83a93d087"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aad1ddaeb7c67d98fd987fdf83a93d087">Mila::Dnn::CudaResidual</a> = <a class="el" href="classMila_1_1Dnn_1_1Residual.html">Residual</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:aad1ddaeb7c67d98fd987fdf83a93d087"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based residual module with customizable tensor types.  <br /></td></tr>
<tr class="separator:aad1ddaeb7c67d98fd987fdf83a93d087"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d7a27be85890bd4493df7a05026c94d" id="r_a7d7a27be85890bd4493df7a05026c94d"><td class="memTemplParams" colspan="2">template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </td></tr>
<tr class="memitem:a7d7a27be85890bd4493df7a05026c94d"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a7d7a27be85890bd4493df7a05026c94d">Mila::Dnn::CudaSoftmax</a> = <a class="el" href="classMila_1_1Dnn_1_1Softmax.html">Softmax</a>&lt; DeviceType::Cuda, TInput, TOutput &gt;</td></tr>
<tr class="memdesc:a7d7a27be85890bd4493df7a05026c94d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based softmax module with customizable tensor types.  <br /></td></tr>
<tr class="separator:a7d7a27be85890bd4493df7a05026c94d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affe8e84e9c24997fe908aa74fcd277e6" id="r_affe8e84e9c24997fe908aa74fcd277e6"><td class="memTemplParams" colspan="2">template&lt;typename TDataType  = float&gt; </td></tr>
<tr class="memitem:affe8e84e9c24997fe908aa74fcd277e6"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#affe8e84e9c24997fe908aa74fcd277e6">Mila::Dnn::CudaTransformerBlock</a> = <a class="el" href="classMila_1_1Dnn_1_1TransformerBlock.html">TransformerBlock</a>&lt; DeviceType::Cuda, TDataType &gt;</td></tr>
<tr class="memdesc:affe8e84e9c24997fe908aa74fcd277e6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for CUDA-based transformer block with customizable tensor type.  <br /></td></tr>
<tr class="separator:affe8e84e9c24997fe908aa74fcd277e6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ada17ef96adb0603d890f78ac5496764c" id="r_ada17ef96adb0603d890f78ac5496764c"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:ada17ef96adb0603d890f78ac5496764c"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#ada17ef96adb0603d890f78ac5496764c">Mila::Dnn::DevicePtr</a> = <a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html">TensorPtr</a>&lt; T, false &gt;</td></tr>
<tr class="memdesc:ada17ef96adb0603d890f78ac5496764c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for device memory pointers.  <br /></td></tr>
<tr class="separator:ada17ef96adb0603d890f78ac5496764c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adc9ab719ef02f0b26941d6c910af9e64" id="r_adc9ab719ef02f0b26941d6c910af9e64"><td class="memTemplParams" colspan="2">template&lt;class T &gt; </td></tr>
<tr class="memitem:adc9ab719ef02f0b26941d6c910af9e64"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#adc9ab719ef02f0b26941d6c910af9e64">Mila::Dnn::DeviceTensor</a> = <a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt; T, <a class="el" href="classMila_1_1Dnn_1_1Compute_1_1CudaMemoryResource.html">Compute::CudaMemoryResource</a> &gt;</td></tr>
<tr class="memdesc:adc9ab719ef02f0b26941d6c910af9e64"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a> type that uses device (GPU) memory.  <br /></td></tr>
<tr class="separator:adc9ab719ef02f0b26941d6c910af9e64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adef07cd48aef905112fa31475275ca85" id="r_adef07cd48aef905112fa31475275ca85"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:adef07cd48aef905112fa31475275ca85"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#adef07cd48aef905112fa31475275ca85">Mila::Dnn::HostPtr</a> = <a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html">TensorPtr</a>&lt; T, true &gt;</td></tr>
<tr class="memdesc:adef07cd48aef905112fa31475275ca85"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type alias for host memory pointers.  <br /></td></tr>
<tr class="separator:adef07cd48aef905112fa31475275ca85"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afde383107a015717518430d36c30e47d" id="r_afde383107a015717518430d36c30e47d"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:afde383107a015717518430d36c30e47d"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#afde383107a015717518430d36c30e47d">Mila::Dnn::HostTensor</a> = <a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt; T, <a class="el" href="namespaceMila_1_1Dnn_1_1Compute.html#a8c32f3f7cc6e4ef70be850a0cf5b9a62">Compute::HostMemoryResource</a> &gt;</td></tr>
<tr class="memdesc:afde383107a015717518430d36c30e47d"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a> type that uses host (CPU) memory.  <br /></td></tr>
<tr class="separator:afde383107a015717518430d36c30e47d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aca917444074e8560d21f87902602dafe" id="r_aca917444074e8560d21f87902602dafe"><td class="memTemplParams" colspan="2">template&lt;class T &gt; </td></tr>
<tr class="memitem:aca917444074e8560d21f87902602dafe"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aca917444074e8560d21f87902602dafe">Mila::Dnn::PinnedTensor</a> = <a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt; T, <a class="el" href="classMila_1_1Dnn_1_1Compute_1_1CudaPinnedMemoryResource.html">Compute::CudaPinnedMemoryResource</a> &gt;</td></tr>
<tr class="memdesc:aca917444074e8560d21f87902602dafe"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a> type that uses pinned (page-locked) host memory.  <br /></td></tr>
<tr class="separator:aca917444074e8560d21f87902602dafe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a108c892956e108baed25cab195778a8b" id="r_a108c892956e108baed25cab195778a8b"><td class="memTemplParams" colspan="2">template&lt;class T &gt; </td></tr>
<tr class="memitem:a108c892956e108baed25cab195778a8b"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a108c892956e108baed25cab195778a8b">Mila::Dnn::UniversalTensor</a> = <a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt; T, <a class="el" href="classMila_1_1Dnn_1_1Compute_1_1CudaManagedMemoryResource.html">Compute::CudaManagedMemoryResource</a> &gt;</td></tr>
<tr class="memdesc:a108c892956e108baed25cab195778a8b"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a> type that uses CUDA managed memory accessible from both CPU and GPU.  <br /></td></tr>
<tr class="separator:a108c892956e108baed25cab195778a8b"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="enum-members" name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:aa4713a92b42ac24051d57d757c5486d4" id="r_aa4713a92b42ac24051d57d757c5486d4"><td class="memItemLeft" align="right" valign="top">enum class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4">Mila::Dnn::ActivationType</a> { <br />
&#160;&#160;<a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4a6adf97f83acf6453d4a6a4b1070f3754">None</a>
, <a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4a7bfde445daa113a9903d4eaa43b41e2b">Relu</a>
, <a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4af48cca1c6deaa6a1c34e4ee46954cf0b">Gelu</a>
, <a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4a17aeea3715b4cdfdf861f237f4011edf">Silu</a>
, <br />
&#160;&#160;<a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4acc132a41cab5676334f353a22a0aa5c5">Tanh</a>
, <a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4a21eebb164e4b8b9bcf64fdb4d8d5dff4">Sigmoid</a>
, <a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4aa2b1e805893964ca62ba2b3467b4c3fc">LeakyRelu</a>
, <a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4aeb72370b2ec1d826723f8aa9d818261c">Mish</a>
<br />
 }</td></tr>
<tr class="memdesc:aa4713a92b42ac24051d57d757c5486d4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Enumeration of supported activation function types.  <a href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4">More...</a><br /></td></tr>
<tr class="separator:aa4713a92b42ac24051d57d757c5486d4"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a2ebe74a2625709fa8d7ee67c07ad6657" id="r_a2ebe74a2625709fa8d7ee67c07ad6657"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a2ebe74a2625709fa8d7ee67c07ad6657">Mila::Dnn::activationTypeToString</a> (<a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4">ActivationType</a> type)</td></tr>
<tr class="memdesc:a2ebe74a2625709fa8d7ee67c07ad6657"><td class="mdescLeft">&#160;</td><td class="mdescRight">Converts an ActivationType enum value to its string representation.  <br /></td></tr>
<tr class="separator:a2ebe74a2625709fa8d7ee67c07ad6657"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f2a715466c7cc87030854d6e090fa0c" id="r_a4f2a715466c7cc87030854d6e090fa0c"><td class="memTemplParams" colspan="2">template&lt;typename TElementType , typename MR  = Compute::CpuMemoryResource&gt; <br />
requires ValidTensorType&lt;TElementType&gt; &amp;&amp; std::is_base_of_v&lt;<a class="el" href="namespaceMila_1_1Dnn_1_1Compute.html#a7b8ae76f48eea1ab92fc10a619f1b666">Compute::MemoryResource</a>, MR&gt;</td></tr>
<tr class="memitem:a4f2a715466c7cc87030854d6e090fa0c"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a4f2a715466c7cc87030854d6e090fa0c">Mila::Dnn::random</a> (<a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt; TElementType, MR &gt; &amp;tensor, TElementType min, TElementType max)</td></tr>
<tr class="memdesc:a4f2a715466c7cc87030854d6e090fa0c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initializes a tensor with random values within a specified range.  <br /></td></tr>
<tr class="separator:a4f2a715466c7cc87030854d6e090fa0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4fdc28fe3efe2837e37c71028e5c2df2" id="r_a4fdc28fe3efe2837e37c71028e5c2df2"><td class="memTemplParams" colspan="2">template&lt;typename T , bool IsHostAccessible&gt; </td></tr>
<tr class="memitem:a4fdc28fe3efe2837e37c71028e5c2df2"><td class="memTemplItemLeft" align="right" valign="top">T *&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a4fdc28fe3efe2837e37c71028e5c2df2">Mila::Dnn::raw_pointer_cast</a> (const <a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html">TensorPtr</a>&lt; T, IsHostAccessible &gt; &amp;ptr) noexcept</td></tr>
<tr class="memdesc:a4fdc28fe3efe2837e37c71028e5c2df2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets a raw pointer from a <a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html" title="Base tensor pointer class that wraps a raw pointer with memory-type safety.">TensorPtr</a> (similar to thrust::raw_pointer_cast)  <br /></td></tr>
<tr class="separator:a4fdc28fe3efe2837e37c71028e5c2df2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6cdacef883b57bfa5d037e963ed964d3" id="r_a6cdacef883b57bfa5d037e963ed964d3"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4">ActivationType</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a6cdacef883b57bfa5d037e963ed964d3">Mila::Dnn::stringToActivationType</a> (const std::string &amp;name)</td></tr>
<tr class="memdesc:a6cdacef883b57bfa5d037e963ed964d3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Converts a string to its corresponding ActivationType enum value.  <br /></td></tr>
<tr class="separator:a6cdacef883b57bfa5d037e963ed964d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3ca6fefd4319c4d00bfd3559be4ae732" id="r_a3ca6fefd4319c4d00bfd3559be4ae732"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a3ca6fefd4319c4d00bfd3559be4ae732"><td class="memTemplItemLeft" align="right" valign="top">constexpr std::string_view&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#a3ca6fefd4319c4d00bfd3559be4ae732">Mila::Dnn::tensor_type_name</a> ()</td></tr>
<tr class="memdesc:a3ca6fefd4319c4d00bfd3559be4ae732"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the string representation of a tensor element type.  <br /></td></tr>
<tr class="separator:a3ca6fefd4319c4d00bfd3559be4ae732"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abc9f67b48a98b5cbadb74ac899def9c4" id="r_abc9f67b48a98b5cbadb74ac899def9c4"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:abc9f67b48a98b5cbadb74ac899def9c4"><td class="memTemplItemLeft" align="right" valign="top">constexpr size_t&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#abc9f67b48a98b5cbadb74ac899def9c4">Mila::Dnn::tensor_type_size</a> ()</td></tr>
<tr class="memdesc:abc9f67b48a98b5cbadb74ac899def9c4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the size in bytes of a tensor element type.  <br /></td></tr>
<tr class="separator:abc9f67b48a98b5cbadb74ac899def9c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aae1fd63419a1d03a4a04034d50402fa1" id="r_aae1fd63419a1d03a4a04034d50402fa1"><td class="memTemplParams" colspan="2">template&lt;typename TElementType , typename MR &gt; <br />
requires ValidTensorType&lt;TElementType&gt;&amp;&amp; std::is_base_of_v&lt;<a class="el" href="namespaceMila_1_1Dnn_1_1Compute.html#a7b8ae76f48eea1ab92fc10a619f1b666">Compute::MemoryResource</a>, MR&gt;</td></tr>
<tr class="memitem:aae1fd63419a1d03a4a04034d50402fa1"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespaceMila_1_1Dnn.html#aae1fd63419a1d03a4a04034d50402fa1">Mila::Dnn::xavier</a> (<a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt; TElementType, MR &gt; &amp;tensor, size_t input_size, size_t output_size)</td></tr>
<tr class="memdesc:aae1fd63419a1d03a4a04034d50402fa1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initializes a tensor with Xavier/Glorot uniform initialization.  <br /></td></tr>
<tr class="separator:aae1fd63419a1d03a4a04034d50402fa1"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="aa220de50dd4be66d488fc2fb76b6b5de" name="aa220de50dd4be66d488fc2fb76b6b5de"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa220de50dd4be66d488fc2fb76b6b5de">&#9670;&#160;</a></span>CpuCompositeModule</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TDataType  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aa220de50dd4be66d488fc2fb76b6b5de">Mila::Dnn::CpuCompositeModule</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1CompositeModule.html">CompositeModule</a>&lt;DeviceType::Cpu, TDataType&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a4dd9edcd0c4bd13d63d7fbdc8e5886af" name="a4dd9edcd0c4bd13d63d7fbdc8e5886af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4dd9edcd0c4bd13d63d7fbdc8e5886af">&#9670;&#160;</a></span>CpuCrossEntropy</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TLogits  = float, typename TTargets  = int&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a4dd9edcd0c4bd13d63d7fbdc8e5886af">Mila::Dnn::CpuCrossEntropy</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1CrossEntropy.html">CrossEntropy</a>&lt;DeviceType::Cpu, TLogits, TTargets&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based cross entropy module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TLogits</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input logits tensor elements. </td></tr>
    <tr><td class="paramname">TTargets</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the target indices, typically int. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a1f196b598ccd007e5aadc2328089f847" name="a1f196b598ccd007e5aadc2328089f847"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1f196b598ccd007e5aadc2328089f847">&#9670;&#160;</a></span>CpuDropout</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a1f196b598ccd007e5aadc2328089f847">Mila::Dnn::CpuDropout</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Dropout.html">Dropout</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based dropout module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a40ef87d5b79c7b0dee7cbc489507c8b3" name="a40ef87d5b79c7b0dee7cbc489507c8b3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a40ef87d5b79c7b0dee7cbc489507c8b3">&#9670;&#160;</a></span>CpuEncoder</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = int, typename TOutput  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a40ef87d5b79c7b0dee7cbc489507c8b3">Mila::Dnn::CpuEncoder</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Encoder.html">Encoder</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based encoder module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input token IDs (typically int). </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output embeddings (typically float). </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aa5a5e7e259089aa0242b266ff0d67a8a" name="aa5a5e7e259089aa0242b266ff0d67a8a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa5a5e7e259089aa0242b266ff0d67a8a">&#9670;&#160;</a></span>CpuGelu</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TDataType  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aa5a5e7e259089aa0242b266ff0d67a8a">Mila::Dnn::CpuGelu</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Gelu.html">Gelu</a>&lt;DeviceType::Cpu, TDataType&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-specific GELU module. </p>
<p>Convenience type that pre-configures the <a class="el" href="classMila_1_1Dnn_1_1Gelu.html" title="Gaussian Error Linear Unit (GELU) activation function module.">Gelu</a> template for CPU execution.</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TDataType</td><td>Floating-point data type (default: float) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aae4780e5db165f896a415e33a6cb5498" name="aae4780e5db165f896a415e33a6cb5498"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aae4780e5db165f896a415e33a6cb5498">&#9670;&#160;</a></span>CpuLayerNorm</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aae4780e5db165f896a415e33a6cb5498">Mila::Dnn::CpuLayerNorm</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1LayerNorm.html">LayerNorm</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based layer normalization module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ae99a94869c61be63b8d7760e0bd82a18" name="ae99a94869c61be63b8d7760e0bd82a18"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae99a94869c61be63b8d7760e0bd82a18">&#9670;&#160;</a></span>CpuLinear</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#ae99a94869c61be63b8d7760e0bd82a18">Mila::Dnn::CpuLinear</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Linear.html">Linear</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based linear module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a5a9b95dc4b57e7427b3874f5e9464e04" name="a5a9b95dc4b57e7427b3874f5e9464e04"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5a9b95dc4b57e7427b3874f5e9464e04">&#9670;&#160;</a></span>CpuMLP</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TDataType  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a5a9b95dc4b57e7427b3874f5e9464e04">Mila::Dnn::CpuMLP</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1MLP.html">MLP</a>&lt;DeviceType::Cpu, TDataType&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based <a class="el" href="classMila_1_1Dnn_1_1MLP.html" title="Multi-Layer Perceptron (MLP) block for neural networks.">MLP</a> module with customizable tensor type. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TDataType</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the tensor elements. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aa8b6fafaecb0ac362383aa6ae4e3726c" name="aa8b6fafaecb0ac362383aa6ae4e3726c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa8b6fafaecb0ac362383aa6ae4e3726c">&#9670;&#160;</a></span>CpuModel</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aa8b6fafaecb0ac362383aa6ae4e3726c">Mila::Dnn::CpuModel</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Model.html">Model</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based models with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="adf61255fac3ae920623d526c30697905" name="adf61255fac3ae920623d526c30697905"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adf61255fac3ae920623d526c30697905">&#9670;&#160;</a></span>CpuModule</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#adf61255fac3ae920623d526c30697905">Mila::Dnn::CpuModule</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Module.html">Module</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based modules with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
    <tr><td class="paramname">TCompute</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type used for internal calculations, defaults to TOutput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a9b2116c65236674a435be3299e8ace6e" name="a9b2116c65236674a435be3299e8ace6e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9b2116c65236674a435be3299e8ace6e">&#9670;&#160;</a></span>CpuMultiHeadAttention</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a9b2116c65236674a435be3299e8ace6e">Mila::Dnn::CpuMultiHeadAttention</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html">MultiHeadAttention</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based multi-head attention module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a10a8ee611a881909837671539c932388" name="a10a8ee611a881909837671539c932388"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a10a8ee611a881909837671539c932388">&#9670;&#160;</a></span>CpuResidual</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a10a8ee611a881909837671539c932388">Mila::Dnn::CpuResidual</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Residual.html">Residual</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based residual module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a8718f8ff56431457fba8f0003606e773" name="a8718f8ff56431457fba8f0003606e773"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8718f8ff56431457fba8f0003606e773">&#9670;&#160;</a></span>CpuSoftmax</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a8718f8ff56431457fba8f0003606e773">Mila::Dnn::CpuSoftmax</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Softmax.html">Softmax</a>&lt;DeviceType::Cpu, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based softmax module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a3adb5958a1b0110973c2a4a08a3a4e33" name="a3adb5958a1b0110973c2a4a08a3a4e33"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3adb5958a1b0110973c2a4a08a3a4e33">&#9670;&#160;</a></span>CpuTransformerBlock</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TDataType  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a3adb5958a1b0110973c2a4a08a3a4e33">Mila::Dnn::CpuTransformerBlock</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1TransformerBlock.html">TransformerBlock</a>&lt;DeviceType::Cpu, TDataType&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CPU-based transformer block with customizable tensor type. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TDataType</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type used for tensor elements throughout the network. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a2c2bcc0d6541dc10d39803ab17be444b" name="a2c2bcc0d6541dc10d39803ab17be444b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2c2bcc0d6541dc10d39803ab17be444b">&#9670;&#160;</a></span>CudaCompositeModule</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TDataType  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a2c2bcc0d6541dc10d39803ab17be444b">Mila::Dnn::CudaCompositeModule</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1CompositeModule.html">CompositeModule</a>&lt;DeviceType::Cuda, TDataType&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="adc98eac63b0fbae2dee52f520cbd4cab" name="adc98eac63b0fbae2dee52f520cbd4cab"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adc98eac63b0fbae2dee52f520cbd4cab">&#9670;&#160;</a></span>CudaCrossEntropy</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TLogits  = float, typename TTargets  = int&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#adc98eac63b0fbae2dee52f520cbd4cab">Mila::Dnn::CudaCrossEntropy</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1CrossEntropy.html">CrossEntropy</a>&lt;DeviceType::Cuda, TLogits, TTargets&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based cross entropy module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TLogits</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input logits tensor elements. </td></tr>
    <tr><td class="paramname">TTargets</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the target indices, typically int. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ac54ac1ac346f0ff778124fb877b6b705" name="ac54ac1ac346f0ff778124fb877b6b705"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac54ac1ac346f0ff778124fb877b6b705">&#9670;&#160;</a></span>CudaDropout</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#ac54ac1ac346f0ff778124fb877b6b705">Mila::Dnn::CudaDropout</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Dropout.html">Dropout</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based dropout module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="af04571a3ff5b48f119d4a3e7041a624d" name="af04571a3ff5b48f119d4a3e7041a624d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af04571a3ff5b48f119d4a3e7041a624d">&#9670;&#160;</a></span>CudaEncoder</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = int, typename TOutput  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#af04571a3ff5b48f119d4a3e7041a624d">Mila::Dnn::CudaEncoder</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Encoder.html">Encoder</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based encoder module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input token IDs (typically int). </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output embeddings (typically float). </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a8b1fe13574bd1bccef7e35c78a3cb552" name="a8b1fe13574bd1bccef7e35c78a3cb552"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8b1fe13574bd1bccef7e35c78a3cb552">&#9670;&#160;</a></span>CudaGelu</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TDataType  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a8b1fe13574bd1bccef7e35c78a3cb552">Mila::Dnn::CudaGelu</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Gelu.html">Gelu</a>&lt;DeviceType::Cuda, TDataType&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-specific GELU module. </p>
<p>Convenience type that pre-configures the <a class="el" href="classMila_1_1Dnn_1_1Gelu.html" title="Gaussian Error Linear Unit (GELU) activation function module.">Gelu</a> template for CUDA GPU execution.</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TDataType</td><td>Floating-point data type (default: float) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aac3f80732055041758726aabd8de167b" name="aac3f80732055041758726aabd8de167b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aac3f80732055041758726aabd8de167b">&#9670;&#160;</a></span>CudaLayerNorm</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aac3f80732055041758726aabd8de167b">Mila::Dnn::CudaLayerNorm</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1LayerNorm.html">LayerNorm</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based layer normalization module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a542a877503e188fefa7c7f45f2511d4a" name="a542a877503e188fefa7c7f45f2511d4a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a542a877503e188fefa7c7f45f2511d4a">&#9670;&#160;</a></span>CudaLinear</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a542a877503e188fefa7c7f45f2511d4a">Mila::Dnn::CudaLinear</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Linear.html">Linear</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based linear module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a5365ad3f092eab6c6a39bd7a6678b8ad" name="a5365ad3f092eab6c6a39bd7a6678b8ad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5365ad3f092eab6c6a39bd7a6678b8ad">&#9670;&#160;</a></span>CudaMLP</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TDataType  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a5365ad3f092eab6c6a39bd7a6678b8ad">Mila::Dnn::CudaMLP</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1MLP.html">MLP</a>&lt;DeviceType::Cuda, TDataType&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based <a class="el" href="classMila_1_1Dnn_1_1MLP.html" title="Multi-Layer Perceptron (MLP) block for neural networks.">MLP</a> module with customizable tensor type. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TDataType</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the tensor elements. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ae1a82f21a2c7cb10ad057cae47f25af9" name="ae1a82f21a2c7cb10ad057cae47f25af9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae1a82f21a2c7cb10ad057cae47f25af9">&#9670;&#160;</a></span>CudaModel</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#ae1a82f21a2c7cb10ad057cae47f25af9">Mila::Dnn::CudaModel</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Model.html">Model</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based models with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aae5155afe2c959b7e2d847e25bb0b4fd" name="aae5155afe2c959b7e2d847e25bb0b4fd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aae5155afe2c959b7e2d847e25bb0b4fd">&#9670;&#160;</a></span>CudaModule</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aae5155afe2c959b7e2d847e25bb0b4fd">Mila::Dnn::CudaModule</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Module.html">Module</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based modules with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
    <tr><td class="paramname">TCompute</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type used for internal calculations, defaults to TOutput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aadfb6cc6bf7e0df44ea0632a1c9243a0" name="aadfb6cc6bf7e0df44ea0632a1c9243a0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aadfb6cc6bf7e0df44ea0632a1c9243a0">&#9670;&#160;</a></span>CudaMultiHeadAttention</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aadfb6cc6bf7e0df44ea0632a1c9243a0">Mila::Dnn::CudaMultiHeadAttention</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1MultiHeadAttention.html">MultiHeadAttention</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based multi-head attention module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aad1ddaeb7c67d98fd987fdf83a93d087" name="aad1ddaeb7c67d98fd987fdf83a93d087"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad1ddaeb7c67d98fd987fdf83a93d087">&#9670;&#160;</a></span>CudaResidual</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aad1ddaeb7c67d98fd987fdf83a93d087">Mila::Dnn::CudaResidual</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Residual.html">Residual</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based residual module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a7d7a27be85890bd4493df7a05026c94d" name="a7d7a27be85890bd4493df7a05026c94d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7d7a27be85890bd4493df7a05026c94d">&#9670;&#160;</a></span>CudaSoftmax</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TInput  = float, typename TOutput  = TInput&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a7d7a27be85890bd4493df7a05026c94d">Mila::Dnn::CudaSoftmax</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Softmax.html">Softmax</a>&lt;DeviceType::Cuda, TInput, TOutput&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based softmax module with customizable tensor types. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TInput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the input tensor elements. </td></tr>
    <tr><td class="paramname">TOutput</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type of the output tensor elements, defaults to TInput. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="affe8e84e9c24997fe908aa74fcd277e6" name="affe8e84e9c24997fe908aa74fcd277e6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#affe8e84e9c24997fe908aa74fcd277e6">&#9670;&#160;</a></span>CudaTransformerBlock</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TDataType  = float&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#affe8e84e9c24997fe908aa74fcd277e6">Mila::Dnn::CudaTransformerBlock</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1TransformerBlock.html">TransformerBlock</a>&lt;DeviceType::Cuda, TDataType&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for CUDA-based transformer block with customizable tensor type. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TDataType</td><td><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> type used for tensor elements throughout the network. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ada17ef96adb0603d890f78ac5496764c" name="ada17ef96adb0603d890f78ac5496764c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ada17ef96adb0603d890f78ac5496764c">&#9670;&#160;</a></span>DevicePtr</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#ada17ef96adb0603d890f78ac5496764c">Mila::Dnn::DevicePtr</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html">TensorPtr</a>&lt;T, false&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for device memory pointers. </p>

</div>
</div>
<a id="adc9ab719ef02f0b26941d6c910af9e64" name="adc9ab719ef02f0b26941d6c910af9e64"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adc9ab719ef02f0b26941d6c910af9e64">&#9670;&#160;</a></span>DeviceTensor</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#adc9ab719ef02f0b26941d6c910af9e64">Mila::Dnn::DeviceTensor</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt;T, <a class="el" href="classMila_1_1Dnn_1_1Compute_1_1CudaMemoryResource.html">Compute::CudaMemoryResource</a>&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a> type that uses device (GPU) memory. </p>
<p>DeviceTensor stores data in GPU memory for optimal performance with CUDA operations. This type is suitable for:</p><ul>
<li>Neural network weights, activations, and gradients</li>
<li><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> used in compute-intensive GPU operations</li>
<li>Performance-critical processing paths</li>
</ul>
<p>Memory safety:</p><ul>
<li>Cannot be directly accessed from host code</li>
<li>Attempting direct element access will trigger runtime errors</li>
<li>Must use to&lt;HostMemoryResource&gt;() to create a host-accessible copy</li>
<li>Safe for use with CUDA kernels through raw_data() method</li>
</ul>
<p>Performance considerations:</p><ul>
<li>Fastest for GPU operations</li>
<li>Requires explicit memory transfers for host access</li>
<li>Most efficient when kept on device throughout processing</li>
</ul>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TPrecision</td><td>The data type of the tensor elements. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="adef07cd48aef905112fa31475275ca85" name="adef07cd48aef905112fa31475275ca85"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adef07cd48aef905112fa31475275ca85">&#9670;&#160;</a></span>HostPtr</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#adef07cd48aef905112fa31475275ca85">Mila::Dnn::HostPtr</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html">TensorPtr</a>&lt;T, true&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Type alias for host memory pointers. </p>

</div>
</div>
<a id="afde383107a015717518430d36c30e47d" name="afde383107a015717518430d36c30e47d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afde383107a015717518430d36c30e47d">&#9670;&#160;</a></span>HostTensor</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#afde383107a015717518430d36c30e47d">Mila::Dnn::HostTensor</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt;T, <a class="el" href="namespaceMila_1_1Dnn_1_1Compute.html#a8c32f3f7cc6e4ef70be850a0cf5b9a62">Compute::HostMemoryResource</a>&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a> type that uses host (CPU) memory. </p>
<p>HostTensor stores data in regular CPU memory that is directly accessible from host code. This type is suitable for:</p><ul>
<li><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> that needs frequent host-side access</li>
<li>Input/output data processing</li>
<li>Debugging and inspection of tensor contents</li>
<li>Operations that primarily run on CPU</li>
</ul>
<p>Memory safety:</p><ul>
<li>Safe to access directly through data() method, operator[], at() method</li>
<li>Direct dereference operations will work correctly</li>
<li>No memory transfers required for host access</li>
</ul>
<p>Performance considerations:</p><ul>
<li>Fast host access, but slower for GPU operations</li>
<li>Requires memory transfers when used with GPU operations</li>
</ul>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TPrecision</td><td>The data type of the tensor elements. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aca917444074e8560d21f87902602dafe" name="aca917444074e8560d21f87902602dafe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aca917444074e8560d21f87902602dafe">&#9670;&#160;</a></span>PinnedTensor</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#aca917444074e8560d21f87902602dafe">Mila::Dnn::PinnedTensor</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt;T, <a class="el" href="classMila_1_1Dnn_1_1Compute_1_1CudaPinnedMemoryResource.html">Compute::CudaPinnedMemoryResource</a>&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a> type that uses pinned (page-locked) host memory. </p>
<p>PinnedTensor stores data in page-locked host memory that cannot be swapped to disk. This type is suitable for:</p><ul>
<li><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> that needs to be frequently transferred between CPU and GPU</li>
<li>Input tensors that will be copied to GPU</li>
<li>Output tensors that need to be read back from GPU</li>
</ul>
<p>Memory safety:</p><ul>
<li>Safe to access directly from host code (data(), operator[], at())</li>
<li>Provides direct dereference operations like HostTensor</li>
<li>No runtime safety issues for host access</li>
</ul>
<p>Performance considerations:</p><ul>
<li>Faster host-device transfers than regular host memory</li>
<li>Consumes a limited system resource (pinned memory)</li>
<li>Should be used judiciously as excessive use can degrade system performance</li>
<li>Host access is typically slower than regular host memory</li>
</ul>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TPrecision</td><td>The data type of the tensor elements. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a108c892956e108baed25cab195778a8b" name="a108c892956e108baed25cab195778a8b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a108c892956e108baed25cab195778a8b">&#9670;&#160;</a></span>UniversalTensor</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespaceMila_1_1Dnn.html#a108c892956e108baed25cab195778a8b">Mila::Dnn::UniversalTensor</a> = typedef <a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt;T, <a class="el" href="classMila_1_1Dnn_1_1Compute_1_1CudaManagedMemoryResource.html">Compute::CudaManagedMemoryResource</a>&gt;</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a> type that uses CUDA managed memory accessible from both CPU and GPU. </p>
<p>UniversalTensor uses CUDA's Unified Memory, which is automatically migrated between host and device as needed by the CUDA runtime. This type is suitable for:</p><ul>
<li><a class="el" href="namespaceMila_1_1Dnn_1_1Data.html">Data</a> that needs to be accessed from both host and device code</li>
<li>Prototyping and development where memory management simplicity is preferred</li>
<li>Cases where optimal data placement isn't known in advance</li>
</ul>
<p>Memory safety:</p><ul>
<li>Safe to access from both host and device code</li>
<li>No explicit memory transfers needed</li>
<li>Provides the simplest programming model with automatic data migration</li>
</ul>
<p>Performance considerations:</p><ul>
<li>More convenient but typically lower performance than explicit memory management</li>
<li>Access patterns that frequently alternate between CPU and GPU may cause thrashing</li>
<li>Best used with CUDA devices that support hardware page faulting (Pascal or newer)</li>
<li>May incur overhead from the runtime system managing page migrations</li>
</ul>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TPrecision</td><td>The data type of the tensor elements. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Enumeration Type Documentation</h2>
<a id="aa4713a92b42ac24051d57d757c5486d4" name="aa4713a92b42ac24051d57d757c5486d4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa4713a92b42ac24051d57d757c5486d4">&#9670;&#160;</a></span>ActivationType</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum class <a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4">Mila::Dnn::ActivationType</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span><span class="mlabel">strong</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Enumeration of supported activation function types. </p>
<p>This enum class defines the different activation functions that can be used throughout the <a class="el" href="namespaceMila.html">Mila</a> library, particularly in neural network layers. </p>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="aa4713a92b42ac24051d57d757c5486d4a6adf97f83acf6453d4a6a4b1070f3754" name="aa4713a92b42ac24051d57d757c5486d4a6adf97f83acf6453d4a6a4b1070f3754"></a>None&#160;</td><td class="fielddoc"><p>No activation (identity function) </p>
</td></tr>
<tr><td class="fieldname"><a id="aa4713a92b42ac24051d57d757c5486d4a7bfde445daa113a9903d4eaa43b41e2b" name="aa4713a92b42ac24051d57d757c5486d4a7bfde445daa113a9903d4eaa43b41e2b"></a>Relu&#160;</td><td class="fielddoc"><p>Rectified <a class="el" href="classMila_1_1Dnn_1_1Linear.html" title="A class representing a linear transformation module.">Linear</a> Unit: max(0, x) </p>
</td></tr>
<tr><td class="fieldname"><a id="aa4713a92b42ac24051d57d757c5486d4af48cca1c6deaa6a1c34e4ee46954cf0b" name="aa4713a92b42ac24051d57d757c5486d4af48cca1c6deaa6a1c34e4ee46954cf0b"></a>Gelu&#160;</td><td class="fielddoc"><p>Gaussian Error <a class="el" href="classMila_1_1Dnn_1_1Linear.html" title="A class representing a linear transformation module.">Linear</a> Unit: x * phi(x) where phi() is the standard Gaussian CDF. </p>
</td></tr>
<tr><td class="fieldname"><a id="aa4713a92b42ac24051d57d757c5486d4a17aeea3715b4cdfdf861f237f4011edf" name="aa4713a92b42ac24051d57d757c5486d4a17aeea3715b4cdfdf861f237f4011edf"></a>Silu&#160;</td><td class="fielddoc"><p>Sigmoid <a class="el" href="classMila_1_1Dnn_1_1Linear.html" title="A class representing a linear transformation module.">Linear</a> Unit (Swish): x * sigmoid(x) </p>
</td></tr>
<tr><td class="fieldname"><a id="aa4713a92b42ac24051d57d757c5486d4acc132a41cab5676334f353a22a0aa5c5" name="aa4713a92b42ac24051d57d757c5486d4acc132a41cab5676334f353a22a0aa5c5"></a>Tanh&#160;</td><td class="fielddoc"><p>Hyperbolic Tangent: tanh(x) </p>
</td></tr>
<tr><td class="fieldname"><a id="aa4713a92b42ac24051d57d757c5486d4a21eebb164e4b8b9bcf64fdb4d8d5dff4" name="aa4713a92b42ac24051d57d757c5486d4a21eebb164e4b8b9bcf64fdb4d8d5dff4"></a>Sigmoid&#160;</td><td class="fielddoc"><p>Sigmoid function: 1 / (1 + exp(-x)) </p>
</td></tr>
<tr><td class="fieldname"><a id="aa4713a92b42ac24051d57d757c5486d4aa2b1e805893964ca62ba2b3467b4c3fc" name="aa4713a92b42ac24051d57d757c5486d4aa2b1e805893964ca62ba2b3467b4c3fc"></a>LeakyRelu&#160;</td><td class="fielddoc"><p>Leaky ReLU: max(alpha * x, x) where alpha is typically 0.01. </p>
</td></tr>
<tr><td class="fieldname"><a id="aa4713a92b42ac24051d57d757c5486d4aeb72370b2ec1d826723f8aa9d818261c" name="aa4713a92b42ac24051d57d757c5486d4aeb72370b2ec1d826723f8aa9d818261c"></a>Mish&#160;</td><td class="fielddoc"><p>Mish: x * tanh(softplus(x)) </p>
</td></tr>
</table>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a id="a2ebe74a2625709fa8d7ee67c07ad6657" name="a2ebe74a2625709fa8d7ee67c07ad6657"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2ebe74a2625709fa8d7ee67c07ad6657">&#9670;&#160;</a></span>activationTypeToString()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::string Mila::Dnn::activationTypeToString </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4">ActivationType</a>&#160;</td>
          <td class="paramname"><em>type</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Converts an ActivationType enum value to its string representation. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">type</td><td>The ActivationType to convert </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>std::string The string representation of the activation type </dd></dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceMila_1_1Dnn_a2ebe74a2625709fa8d7ee67c07ad6657_icgraph.png" border="0" usemap="#anamespaceMila_1_1Dnn_a2ebe74a2625709fa8d7ee67c07ad6657_icgraph" alt=""/></div>
<map name="anamespaceMila_1_1Dnn_a2ebe74a2625709fa8d7ee67c07ad6657_icgraph" id="anamespaceMila_1_1Dnn_a2ebe74a2625709fa8d7ee67c07ad6657_icgraph">
<area shape="rect" title="Converts an ActivationType enum value to its string representation." alt="" coords="232,5,419,45"/>
<area shape="rect" href="classMila_1_1Dnn_1_1MLP.html#aee8a9284ec818f02e5ede4cc0552758f" title="Generates a string representation of this module&#39;s configuration." alt="" coords="5,13,184,38"/>
<area shape="poly" title=" " alt="" coords="219,28,184,28,184,23,219,23"/>
</map>
</div>

</div>
</div>
<a id="a4f2a715466c7cc87030854d6e090fa0c" name="a4f2a715466c7cc87030854d6e090fa0c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4f2a715466c7cc87030854d6e090fa0c">&#9670;&#160;</a></span>random()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TElementType , typename MR  = Compute::CpuMemoryResource&gt; <br />
requires ValidTensorType&lt;TElementType&gt; &amp;&amp; std::is_base_of_v&lt;<a class="el" href="namespaceMila_1_1Dnn_1_1Compute.html#a7b8ae76f48eea1ab92fc10a619f1b666">Compute::MemoryResource</a>, MR&gt;</div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void Mila::Dnn::random </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt; TElementType, MR &gt; &amp;&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">TElementType&#160;</td>
          <td class="paramname"><em>min</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">TElementType&#160;</td>
          <td class="paramname"><em>max</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Initializes a tensor with random values within a specified range. </p>
<p>This function populates a tensor with random floating-point values uniformly distributed between the specified minimum and maximum values. It handles both host and device memory resources appropriately, copying data to the host for initialization if needed.</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TElementType</td><td>The element data type of the tensor (float, half, etc.) </td></tr>
    <tr><td class="paramname">MR</td><td>The memory resource type used by the tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to initialize with random values </td></tr>
    <tr><td class="paramname">min</td><td>The minimum value for the random distribution </td></tr>
    <tr><td class="paramname">max</td><td>The maximum value for the random distribution</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd>Uses a fixed seed (42) for reproducible results rather than truly random values </dd></dl>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceMila_1_1Dnn_a4f2a715466c7cc87030854d6e090fa0c_cgraph.png" border="0" usemap="#anamespaceMila_1_1Dnn_a4f2a715466c7cc87030854d6e090fa0c_cgraph" alt=""/></div>
<map name="anamespaceMila_1_1Dnn_a4f2a715466c7cc87030854d6e090fa0c_cgraph" id="anamespaceMila_1_1Dnn_a4f2a715466c7cc87030854d6e090fa0c_cgraph">
<area shape="rect" title="Initializes a tensor with random values within a specified range." alt="" coords="5,42,148,67"/>
<area shape="rect" href="classMila_1_1Dnn_1_1Tensor.html#a4f707ea4c0f00aab58189071fa1c11fd" title="Gets a raw pointer to the tensor data for use in CUDA kernels." alt="" coords="209,5,351,45"/>
<area shape="poly" title=" " alt="" coords="148,42,195,35,195,40,149,47"/>
<area shape="rect" href="classMila_1_1Dnn_1_1Tensor.html#af952a34808a4ad53bc9588854ddd8592" title=" " alt="" coords="196,70,364,95"/>
<area shape="poly" title=" " alt="" coords="149,62,183,67,182,72,148,67"/>
</map>
</div>

</div>
</div>
<a id="a4fdc28fe3efe2837e37c71028e5c2df2" name="a4fdc28fe3efe2837e37c71028e5c2df2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4fdc28fe3efe2837e37c71028e5c2df2">&#9670;&#160;</a></span>raw_pointer_cast()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T , bool IsHostAccessible&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">T * Mila::Dnn::raw_pointer_cast </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html">TensorPtr</a>&lt; T, IsHostAccessible &gt; &amp;&#160;</td>
          <td class="paramname"><em>ptr</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Gets a raw pointer from a <a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html" title="Base tensor pointer class that wraps a raw pointer with memory-type safety.">TensorPtr</a> (similar to thrust::raw_pointer_cast) </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TPrecision</td><td>Element type </td></tr>
    <tr><td class="paramname">IsHostAccessible</td><td>Whether the pointer is host-accessible </td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">ptr</td><td><a class="el" href="classMila_1_1Dnn_1_1TensorPtr.html" title="Base tensor pointer class that wraps a raw pointer with memory-type safety.">TensorPtr</a> to convert </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>TPrecision* Raw pointer </dd></dl>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceMila_1_1Dnn_a4fdc28fe3efe2837e37c71028e5c2df2_cgraph.png" border="0" usemap="#anamespaceMila_1_1Dnn_a4fdc28fe3efe2837e37c71028e5c2df2_cgraph" alt=""/></div>
<map name="anamespaceMila_1_1Dnn_a4fdc28fe3efe2837e37c71028e5c2df2_cgraph" id="anamespaceMila_1_1Dnn_a4fdc28fe3efe2837e37c71028e5c2df2_cgraph">
<area shape="rect" title="Gets a raw pointer from a TensorPtr (similar to thrust::raw_pointer_cast)" alt="" coords="5,5,208,31"/>
<area shape="rect" href="classMila_1_1Dnn_1_1TensorPtr.html#a3e030c89995b3f9d8b8300764ab997fa" title="Gets the raw pointer (explicit conversion)" alt="" coords="256,5,437,31"/>
<area shape="poly" title=" " alt="" coords="208,15,242,15,242,21,208,21"/>
</map>
</div>

</div>
</div>
<a id="a6cdacef883b57bfa5d037e963ed964d3" name="a6cdacef883b57bfa5d037e963ed964d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6cdacef883b57bfa5d037e963ed964d3">&#9670;&#160;</a></span>stringToActivationType()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespaceMila_1_1Dnn.html#aa4713a92b42ac24051d57d757c5486d4">ActivationType</a> Mila::Dnn::stringToActivationType </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>name</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Converts a string to its corresponding ActivationType enum value. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">name</td><td>The string representation of an activation function </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>ActivationType The corresponding enum value </dd></dl>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">std::invalid_argument</td><td>if the string doesn't match any known activation function </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a3ca6fefd4319c4d00bfd3559be4ae732" name="a3ca6fefd4319c4d00bfd3559be4ae732"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3ca6fefd4319c4d00bfd3559be4ae732">&#9670;&#160;</a></span>tensor_type_name()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr std::string_view Mila::Dnn::tensor_type_name </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">constexpr</span><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get the string representation of a tensor element type. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">T</td><td>The tensor element type </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>constexpr std::string_view The type name as a string </dd></dl>

</div>
</div>
<a id="abc9f67b48a98b5cbadb74ac899def9c4" name="abc9f67b48a98b5cbadb74ac899def9c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abc9f67b48a98b5cbadb74ac899def9c4">&#9670;&#160;</a></span>tensor_type_size()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">constexpr size_t Mila::Dnn::tensor_type_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">constexpr</span><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get the size in bytes of a tensor element type. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">T</td><td>The tensor element type </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>constexpr size_t Size in bytes </dd></dl>

</div>
</div>
<a id="aae1fd63419a1d03a4a04034d50402fa1" name="aae1fd63419a1d03a4a04034d50402fa1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aae1fd63419a1d03a4a04034d50402fa1">&#9670;&#160;</a></span>xavier()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename TElementType , typename MR &gt; <br />
requires ValidTensorType&lt;TElementType&gt;&amp;&amp; std::is_base_of_v&lt;<a class="el" href="namespaceMila_1_1Dnn_1_1Compute.html#a7b8ae76f48eea1ab92fc10a619f1b666">Compute::MemoryResource</a>, MR&gt;</div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void Mila::Dnn::xavier </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classMila_1_1Dnn_1_1Tensor.html">Tensor</a>&lt; TElementType, MR &gt; &amp;&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>input_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>output_size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">export</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Initializes a tensor with Xavier/Glorot uniform initialization. </p>
<p>Xavier initialization is a method designed to keep the scale of gradients roughly the same in all layers of a neural network. It initializes weights with values sampled from a uniform distribution with limits calculated as a function of the input and output sizes of the layer.</p>
<p>The distribution range is [-limit, limit] where: limit = sqrt(6 / (input_size + output_size))</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">TElementType</td><td>The element data type of the tensor (float, half, etc.) </td></tr>
    <tr><td class="paramname">MR</td><td>The memory resource type used by the tensor </td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to initialize with Xavier initialization </td></tr>
    <tr><td class="paramname">input_size</td><td>The size of the input dimension </td></tr>
    <tr><td class="paramname">output_size</td><td>The size of the output dimension</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd>Uses a fixed seed (42) for reproducible results rather than truly random values </dd></dl>
<dl class="section see"><dt>See also</dt><dd><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a> </dd></dl>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespaceMila_1_1Dnn_aae1fd63419a1d03a4a04034d50402fa1_cgraph.png" border="0" usemap="#anamespaceMila_1_1Dnn_aae1fd63419a1d03a4a04034d50402fa1_cgraph" alt=""/></div>
<map name="anamespaceMila_1_1Dnn_aae1fd63419a1d03a4a04034d50402fa1_cgraph" id="anamespaceMila_1_1Dnn_aae1fd63419a1d03a4a04034d50402fa1_cgraph">
<area shape="rect" title="Initializes a tensor with Xavier/Glorot uniform initialization." alt="" coords="5,42,139,67"/>
<area shape="rect" href="classMila_1_1Dnn_1_1Tensor.html#a4f707ea4c0f00aab58189071fa1c11fd" title="Gets a raw pointer to the tensor data for use in CUDA kernels." alt="" coords="199,5,342,45"/>
<area shape="poly" title=" " alt="" coords="139,42,185,35,186,40,140,47"/>
<area shape="rect" href="classMila_1_1Dnn_1_1Tensor.html#af952a34808a4ad53bc9588854ddd8592" title=" " alt="" coords="187,70,355,95"/>
<area shape="poly" title=" " alt="" coords="140,61,174,66,173,72,139,67"/>
</map>
</div>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespaceMila.html">Mila</a></li><li class="navelem"><a class="el" href="namespaceMila_1_1Dnn.html">Dnn</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8 </li>
  </ul>
</div>
</body>
</html>
